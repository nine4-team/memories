# Spec Requirements: Voice Story Recording & Processing

## Initial Description
**Voice Story Recording & Processing** — Integrate in-house Flutter dictation plugin; record audio, save to Supabase Storage, process transcription, and transform into narrative format (backend processing); create Story object with title and narrative text.

## Requirements Discussion

### First Round Questions

**Q1:** I’m assuming voice stories get their own dedicated “Record Story” surface accessed from the primary capture button, and once recording finishes we immediately create a `stories` row before transcription completes. Is that correct, or should the recording live inside a broader “Create Story” form where users enter metadata first?  
**Answer:** Capture stays unified: mic + text entry live on one quick-capture screen shared by Moments, Stories, and Mementos. Users dictate first, text lands in the box, then they choose the memory type. We still want raw audio for Stories, but that must not break the shared capture flow—confirming if the plugin can support this without duplicating flows.

**Q2:** I’m thinking we persist both the raw audio file (Supabase Storage, e.g., `stories/audio/{userId}/{storyId}.m4a`) and the processed narrative text, with the Flutter app uploading audio right after recording. Should uploads happen before transcription kicks off, or do you want local caching/offline retries before any backend work starts?  
**Answer:** Raw audio should be persisted for Stories even within the unified capture flow. Offline capture is critical, so both transcript and audio need reliable local storage + retries. Current plugin transcribes immediately; we may need to extend it to surface audio blobs for persistence.

**Q3:** For transcription and narrative shaping, I’m assuming an Edge Function will pull the stored audio, run transcription (OpenAI/Whisper or Apple APIs), then apply narrative formatting (title suggestion, cleaned paragraphs) before updating the `stories` table. Do you want the Flutter app to send any hints (prompt, mood tags) alongside the audio to guide the narrative output?  
**Answer:** No extra hints besides the selected memory type. Backend modules per memory type own all processing, including narrative shaping.

**Q4:** I’m assuming we show live waveform + timer UI (similar to the dictation spec) with controls for cancel, pause/resume, and finish, and we only expose iOS in V1 because the plugin’s native path is iOS-focused. Should Android receive a reduced experience using the legacy plugin, or is Android support out of scope for this release?  
**Answer:** UI mirrors the plugin README: text field, mic, waveform, X (cancel) on left, checkmark (save) on right, transcription dumps into the text box on confirm. Android is out of scope for now; future parity will come once the native plugin supports it.

**Q5:** After we generate the narrative, I’m assuming users can edit both the title and body before saving, and we version their edits without reprocessing audio. Would you prefer an automated title (e.g., first sentence) with optional manual override, or should we prompt them to type/confirm a title every time?  
**Answer:** Narrative and title are generated by an LLM from the transcript; both are saved automatically but users can edit them afterward without reprocessing audio.

**Q6:** For user feedback, I’m thinking we display a “Processing story…” status with background upload/transcription, then notify when the narrative is ready (toast + automatic navigation to detail view). Should we also provide a retry/failure state if transcription or formatting fails, and keep the raw audio accessible for reprocessing?  
**Answer:** Let users keep using the app while processing happens in the background. Create the Story entry immediately and show it as “processing.” Send a push notification (with deep link) when processing completes if they’re away; show a toast if they’re in-app. Raw audio must remain accessible for retries, and failure states should support reprocessing.

**Q7:** Are there specific scenarios you already want to exclude (e.g., multi-language recording, background recording while app is minimized, attaching photos to stories during this flow, or sharing stories immediately after capture)?  
**Answer:** No exclusions right now. Users should be able to attach media while the story processes, and the story screen should stay interactive.

### Existing Code to Reference

**Similar Features Identified:**  
- Feature: Flutter dictation plugin implementation guidance  
  - Path: `/Users/benjaminmackenzie/Dev/flutter_dictation/README.md`  
  - Components to reuse: unified mic/text entry UI, waveform controls, cancel/check patterns, dictation service lifecycle.  
- Feature: Prior dictation spec reference  
  - Path: `/Users/benjaminmackenzie/Dev/vip/agent-os/specs/2025-11-16-voice-dictation-for-notes`  
  - Components to reuse: question/answer learnings around waveform UX, offline buffering strategies, notification patterns.

### Follow-up Questions

**Follow-up 1:** Raw audio capture pipeline: should we extend the plugin to record PCM locally, write it to disk, and upload (or enqueue) that file ourselves? Any format/retention preferences?  
**Answer:** Currently investigating how to capture raw audio via the plugin; plan is to extend it as needed to surface audio for storage (details TBD).

**Follow-up 2:** Offline queueing order: when offline, should we upload audio before transcript, or the reverse, and is user involvement needed?  
**Answer:** Queue both transcript and audio automatically. When connectivity returns, send transcript first (to start processing sooner) and upload audio afterward; user does not need to take action.

**Follow-up 3:** Memory routing & attachments: can users attach photos/videos before submitting the story, and do attachments follow the same offline queue rules?  
**Answer:** Yes—attachments are available on the unified capture screen before submission, and all memory data (text, audio, media) must support offline queueing for later sync.

## Visual Assets
No visual assets provided.

## Requirements Summary

### Functional Requirements
- Unified capture screen with mic + text field feeds all memory types; stories are chosen after transcription text populates the field.
- Dictation plugin must capture audio, stream transcription into the text box, and expose raw audio files for storage/retries.
- Persist transcripts immediately and store raw audio (Supabase Storage) for every story; offline capture queues both assets.
- Backend processing (per memory type) transforms transcripts into narrative text + LLM-generated titles once submission occurs.
- Users can edit generated narratives and titles post-processing without reprocessing audio.
- Allow media attachments (photos/videos) prior to submission; they sync alongside audio/text and continue uploading while story processing runs in background.
- Story entries show a “processing” state until narrative generation finishes; users can navigate elsewhere meanwhile.
- Push notifications (with deep links) alert users when processing completes; in-app users receive toasts.
- Failure handling retains raw audio and surfaces retry options without losing captured content.

### Reusability Opportunities
- Reuse Flutter dictation plugin UI patterns (waveform, mic controls) per `/Users/benjaminmackenzie/Dev/flutter_dictation/README.md`.
- Reference prior dictation spec learnings for offline buffering, permissions, and waveform UX.
- Leverage unified capture form logic already planned for Moments/Mementos to avoid duplicating forms.

### Scope Boundaries
**In Scope:**
- iOS-only native dictation plugin integration, including raw audio persistence and offline queueing.
- Unified capture UX enhancements (media attachments, type selection, processing states).
- Backend narrative processing pipeline triggered automatically per story submission.
- Push/toast notifications and retry flows tied to story processing lifecycle.

**Out of Scope:**
- Android implementation (until plugin parity exists).
- Special handling for multi-language dictation, profanity filtering, or advanced voice commands.
- Locking the story detail screen during processing (users can continue editing/attaching media).

### Technical Considerations
- Dictation plugin must be extended to surface audio data for storage alongside on-device transcription (iOS first).
- Offline queue managers handle transcripts, audio files, and media attachments, prioritizing transcript upload once connected.
- Supabase Storage pathing for audio must align with `stories` table schema; backend Edge Functions consume audio + transcript per story type.
- LLM prompt for narrative + title runs server-side; editing occurs client-side with optimistic updates.
- Notification system must support push (background) and toast (foreground) with deep links to the story detail.
- Failure states require durable audio storage to enable reprocessing without re-recording.

