# Spec Requirements: Moment Creation (Text + Media)

## Initial Description
**Moment Creation (Text + Media)** — Build form to create Moments with title (required), text description (optional), and ability to upload photos/videos from device; store media in Supabase Storage.

## Requirements Discussion

### First Round Questions

**Q1:** I’m assuming the Moment composer will live on a single full-screen Flutter sheet with a required title field, optional rich text description, and Save/Cancel actions—does that match your vision, or do you prefer a multi-step flow (e.g., first text, then media)?
**Answer:** Title should be derived by the LLM from the user’s dictated description, so the description is the only text input users provide. Make description optional so people can capture media quickly and add description later. Titles remain editable after generation.

**Q2:** I’m thinking we cap Moments at, say, up to 10 photos and 1 video per entry to keep uploads manageable; should we enforce counts like that, or allow unlimited media as long as uploads succeed?
**Answer:** Cap at 10 photos and 3 videos per Moment.

**Q3:** For media capture, should we support both direct camera access and picking from the gallery by default, or focus on one of those first to keep scope tight?
**Answer:** Support both direct camera access and gallery picking as long as the direct camera option doesn’t require heavy native work.

**Q4:** I’m assuming uploads hit Supabase Storage only after the user taps Save, with a progress indicator while we stream files—would you rather start uploading immediately when the user selects media to reduce wait time later?
**Answer:** Upload after Save is fine.

**Q5:** Should the Moment form capture any contextual metadata (location, timestamp overrides, tags) now, or keep this version to just title/description/media and store metadata automatically?
**Answer:** Aim for a shared capture form driven by dictation. Capture timestamp and location passively if possible, and allow freeform tagging in a subtle interface.

**Q6:** Are there any currently planned exclusions (e.g., editing moments post-save, draft autosave, batch tagging) that we should explicitly keep out of this spec?
**Answer:** No specific exclusions; the shared capture form should let people capture media quickly (even without description) and add details later, effectively covering draft needs.

### Existing Code to Reference
No similar existing features identified for reference.

### Follow-up Questions

**Follow-up 1:** For the unified capture screen driven by dictation, how do you want users to classify the memory type—should the screen default to “Moment” with quick toggles for Story/Memento, or should dictation try to infer the type automatically and then let users override?
**Answer:** Default to Moment with quick toggles for Story or Memento.

**Follow-up 2:** Since titles are generated by the LLM from the dictated description but editable afterward, do we need to store both the raw transcript and the finalized LLM-generated title so users can regenerate later, or is a one-time generation sufficient?
**Answer:** Store the raw transcript; that preserves regeneration flexibility.

## Visual Assets
No visual assets provided.

## Requirements Summary

### Functional Requirements
- Unified capture screen centered on voice dictation with optional media attachments.
- Description input optional; title auto-generated from dictation but editable post-save.
- Support up to 10 photos and 3 videos per Moment, with both camera capture and gallery selection.
- Save triggers media uploads to Supabase Storage; passive timestamp and location capture when available.
- Allow subtle freeform tagging and quick toggles to switch between Moment/Story/Memento capture types.
- Store raw voice transcript alongside LLM-generated title.
- Provide offline-safe capture by persisting queued Moments locally and syncing automatically when connectivity returns, with visible queued/sync/error statuses.

### Reusability Opportunities
- Investigate reusable tagging controls and shared capture UI patterns once they exist.
- Potential reuse of storage/upload services and metadata utilities across memory types.

### Scope Boundaries
**In Scope:** Unified dictation-driven capture flow, media attachment caps and handling, passive metadata capture, tagging UI, title generation pipeline, storage of raw transcript and generated title, toggles for memory type.

**Out of Scope:** Draft autosave beyond the “capture now, add description later” behavior; advanced editing workflows; inference-based automatic classification beyond manual toggles; complex native camera integrations.

### Technical Considerations
- Flutter UI with Riverpod state management; leverage in-house dictation plugin for voice capture.
- LLM service generates titles from transcripts; store transcripts for future regeneration.
- Supabase Storage handles media uploads post-save; metadata stored in PostgreSQL with timestamp/location fields.
- Need lightweight tagging data model and passive location capture respecting permissions.
- Implement persistent offline queue storage plus sync workers that resume uploads and reconcile server IDs once online.
